{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EVALUATION = True # Re-compute results (this might take a long time)\n",
    "SAVE_FIGURES = True\n",
    "FIGURE_PATH = \"./figures/\"\n",
    "EVENT_LOGS_PATH = \"../../\"\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "from typing import Tuple, TypedDict, Literal\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pm4py.objects.petri_net.utils.petri_utils import (\n",
    "    remove_transition,\n",
    "    get_transition_by_name,\n",
    ")\n",
    "from pm4py.objects.petri_net.obj import Marking, PetriNet\n",
    "import pm4py\n",
    "# For Alignment MP\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "MP_ALIGNMENTS = False\n",
    "\n",
    "\n",
    "class RemovingActsResult(TypedDict):\n",
    "    removedActs: frozenset[str]\n",
    "    fitness: float\n",
    "    precision: float\n",
    "\n",
    "\n",
    "def get_sorted_disconnected_acts(net: PetriNet, log: pd.DataFrame):\n",
    "    disconnected_acts = {\n",
    "        t.label\n",
    "        for t in net.transitions\n",
    "        if len(t.in_arcs) == 0 and len(t.out_arcs) == 0 and t.label is not None\n",
    "    }\n",
    "    acts_with_event_count = pm4py.stats.get_event_attribute_values(log, \"concept:name\")\n",
    "    sorted_disconnected_acts = [\n",
    "        k\n",
    "        for k, v in sorted(acts_with_event_count.items(), key=lambda item: item[1])\n",
    "        if k in disconnected_acts\n",
    "    ]\n",
    "    return (sorted_disconnected_acts, acts_with_event_count)\n",
    "\n",
    "\n",
    "# Powerset\n",
    "def powerset(l: list):\n",
    "    return itertools.chain.from_iterable(\n",
    "        itertools.combinations(l, r) for r in range(0, len(l) + 1)\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_transitions(\n",
    "    mode: Literal[\"greedy\", \"brute-force\"],\n",
    "    pn: Tuple[PetriNet, Marking, Marking],\n",
    "    log: pd.DataFrame,\n",
    "    max_acts_remove = 5\n",
    "):\n",
    "    (initial_net, init_im, init_fm) = copy.deepcopy(pn)\n",
    "    if init_im is None:\n",
    "        init_im = Marking()\n",
    "    if init_fm is None:\n",
    "        init_fm = Marking()\n",
    "\n",
    "    sorted_disconnected_acts, acts_with_count = get_sorted_disconnected_acts(\n",
    "        initial_net, log\n",
    "    )\n",
    "    # Only consider at most the max_acts_remove most infrequent acts\n",
    "    sorted_disconnected_acts = sorted_disconnected_acts[0:max_acts_remove+1]\n",
    "\n",
    "    if mode == \"greedy\":\n",
    "        # List of the top i most infrequent activities, for i=0...#disconected_acts\n",
    "        transition_sets_to_remove = [\n",
    "            frozenset(sorted_disconnected_acts[0:i])\n",
    "            for i in range(len(sorted_disconnected_acts) + 1)\n",
    "        ]   \n",
    "    else:\n",
    "        # Powerset (i.e., try out all combinations)\n",
    "        transition_sets_to_remove = [\n",
    "            frozenset(s) for s in powerset(sorted_disconnected_acts)\n",
    "        ]\n",
    "    print(f\"#Iterations: {len(transition_sets_to_remove)}\")\n",
    "    removing_disconnected_acts_data: dict[frozenset[str], RemovingActsResult] = dict()\n",
    "    for trans_to_rm in transition_sets_to_remove:\n",
    "        (net, im, fm) = copy.deepcopy((initial_net, init_im, init_fm))\n",
    "        print(im, fm)\n",
    "        if trans_to_rm not in removing_disconnected_acts_data:\n",
    "            for a in trans_to_rm:\n",
    "                # Assumes that label = name for transition, but for our case this should be fine.\n",
    "                t = get_transition_by_name(net, a)\n",
    "                net = remove_transition(net, t)\n",
    "            fit = pm4py.fitness_alignments(log, net, im, fm, MP_ALIGNMENTS)[\n",
    "                \"average_trace_fitness\"\n",
    "            ]\n",
    "            prec = pm4py.precision_alignments(log, net, im, fm, MP_ALIGNMENTS)\n",
    "            print(f\"Fitness: {fit}, Precision: {prec}\")\n",
    "            removing_disconnected_acts_data[trans_to_rm] = {\n",
    "                \"removedActs\": trans_to_rm,\n",
    "                \"fitness\": fit,\n",
    "                \"precision\": prec,\n",
    "            }\n",
    "        # Do not re-do already evaluated configurations\n",
    "        else:\n",
    "            pass\n",
    "    return removing_disconnected_acts_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def dump_json_results(\n",
    "    name: str, removed_acts_data: dict[frozenset[str], RemovingActsResult]\n",
    "):\n",
    "    as_list = [\n",
    "        {\n",
    "            **removed_acts_data[k],\n",
    "            \"removedActs\": list(removed_acts_data[k][\"removedActs\"]),\n",
    "        }\n",
    "        for k in removed_acts_data\n",
    "    ]\n",
    "    with open(FIGURE_PATH + name, \"wt\") as f:\n",
    "        json.dump(as_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_to_path = {\n",
    "    \"BPI_Challenge_2020_DomesticDeclarations\": f\"{EVENT_LOGS_PATH}DomesticDeclarations.xes.gz\",\n",
    "    \"BPI_Challenge_2020\": f\"{EVENT_LOGS_PATH}BPI_Challenge_2020_request_for_payments.xes\",\n",
    "    \"BPI_Challenge_2019_sampled_3000cases\": f\"{EVENT_LOGS_PATH}BPI_Challenge_2019_sampled3000.xes\",\n",
    "    \"Sepsis\": f\"{EVENT_LOGS_PATH}Sepsis Cases - Event Log.xes.gz\",\n",
    "    \"RTFM\": f\"{EVENT_LOGS_PATH}Road_Traffic_Fine_Management_Process.xes.gz\",\n",
    "}\n",
    "log_to_IMf_res = {\n",
    "    \"BPI_Challenge_2020\": \"algo_res2023-04-02T21:05:14.839491_BPI_Challenge_2020.pickle\",\n",
    "    \"BPI_Challenge_2020_DomesticDeclarations\": \"algo_res2023-04-02T21:02:52.613449_BPI_Challenge_2020_DomesticDeclarations.pickle\",\n",
    "    \"RTFM\": \"algo_res2023-04-02T23:26:44.105591_RTFM.pickle\",\n",
    "    \"Sepsis\": \"algo_res2023-04-02T23:11:00.988178_Sepsis.pickle\",\n",
    "    \"BPI_Challenge_2019_sampled_3000cases\": \"algo_res2023-04-02T22:08:45.153238_BPI_Challenge_2019_sampled_3000cases.pickle\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_logs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import warnings\n",
    "ALPHAPPP_PNML_FOLDER = \"../../report/report_figures/\"\n",
    "pn_path_per_algolog = dict()\n",
    "for pnml_path in glob.glob(ALPHAPPP_PNML_FOLDER + \"*.pnml\"):\n",
    "    pnml_name = pnml_path.split(\"/\")[-1]\n",
    "    split_name = pnml_name.split(\"_\")\n",
    "    split_name.pop() # Pop timestamp\n",
    "    algo_name = split_name.pop() # Pop algo name\n",
    "    split_name.pop() # Pop _model\n",
    "    log_name = \"_\".join(split_name)\n",
    "    pn_path_per_algolog[(log_name,algo_name)] = pnml_path\n",
    "    log_path = log_to_path[log_name]\n",
    "    pn = pm4py.read_pnml(pnml_path)\n",
    "    if RUN_EVALUATION:\n",
    "        n = len(pn[0].transitions)\n",
    "\n",
    "        disconnected_acts = {t.label for t in pn[0].transitions if len(t.in_arcs) == 0 and len(t.out_arcs) == 0 and t.label is not None}\n",
    "        k = len(disconnected_acts)\n",
    "        print(f\"{k} out of {n} transitions are disconnected & labeled\")\n",
    "        # print(f\"This means that there are {pow(2,k)} subsets of disconnected & labeled transitions to test in brute-force mode.\")\n",
    "        # print(\"---\")\n",
    "        # if pow(2,k) > 100:\n",
    "        #     print(f\"Skip {pnml_name} because evaluating all subsets is infeasible (#: {pow(2,k)})\")\n",
    "        # elif k <= 1:\n",
    "        #     print(f\"Skip {pnml_name} because there is only 0/1 disconnected labeled transition\")\n",
    "        # else:\n",
    "        if k > 1:\n",
    "            print(f\"Evaluating for {pnml_name}\")\n",
    "            # Load log (if necessary)\n",
    "            if log_name in loaded_logs:\n",
    "                log = loaded_logs[log_name]\n",
    "            else:\n",
    "                log_path = log_to_path[log_name]\n",
    "                with warnings.catch_warnings():\n",
    "                    # Suppress flood of dt-parsing warnings:\n",
    "                    # (Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.)\n",
    "                    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "                    log = pm4py.read_xes(log_path)\n",
    "                loaded_logs[log_name] = log\n",
    "                print(\"Finished importing EventLog\")\n",
    "                \n",
    "            greedy_res = remove_transitions(\"greedy\",pn,log)\n",
    "            dump_json_results(f\"removed-acts-results-{log_name}-{algo_name}-greedy.json\", greedy_res)\n",
    "            print(\"Greedy mode finished\")\n",
    "            brute_force_res = remove_transitions(\"brute-force\",pn,log)\n",
    "            dump_json_results(f\"removed-acts-results-{log_name}-{algo_name}-brute-force.json\", brute_force_res)\n",
    "            print(\"Brute-force mode finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import pickle\n",
    "from typing import Optional\n",
    "import matplotlib\n",
    "MAIN_PALETTE = [\"#1f77b4\",\"#f9bc02\",\"#2ca02c\"]\n",
    "ALT_PALETTE = [\"#41cde0\",\"#fcec3f\",\"#5dfc5d\"]\n",
    "MEASURES = [\"Fitness\",\"F1-Score\",\"Precision\"]\n",
    "def plot_greedy_act_removal(fig_title: str,removing_disconnected_acts_data: list[RemovingActsResult], compare_to: Optional[list[RemovingActsResult]] = None):\n",
    "    plt.rc('legend',fontsize=14)\n",
    "    values = []\n",
    "    comparison_values = []\n",
    "    for i in range(len(removing_disconnected_acts_data)):\n",
    "        # Optionally: Use % of events not covered anymore (bc. activity was removed)\n",
    "        # Note, that this only makes sense when not using compare_to\n",
    "        # Additionally requires log frequency info (i.e., disconnected_acts_count object)\n",
    "        # removed_events = 0\n",
    "        # for a in datum['removedActs']:\n",
    "        #     removed_events += disconnected_acts_count[a]\n",
    "        # removed_events_rel = round(removed_events/len(log)*100,2)\n",
    "        removed_events_rel = i\n",
    "        datum = removing_disconnected_acts_data[i]\n",
    "        fit = datum['fitness']\n",
    "        prec = datum['precision']\n",
    "        f1 = 2*(prec * fit)/(prec + fit)\n",
    "        values.append({'value': fit, \"type\": \"fitness\", \"x\": removed_events_rel})\n",
    "        values.append({'value': f1, \"type\": \"f1-score\", \"x\": removed_events_rel})\n",
    "        values.append({'value': prec, \"type\": \"precision\", \"x\": removed_events_rel})\n",
    "        if compare_to is not None:\n",
    "            removed_events_rel = i #round(removed_events/len(log)*100,2)\n",
    "            datum = compare_to[i]\n",
    "            fit = datum['fitness']\n",
    "            prec = datum['precision']\n",
    "            f1 = 2*(prec * fit)/(prec + fit)\n",
    "            comparison_values.append({'value': fit, \"type\": \"fitness\", \"x\": removed_events_rel})\n",
    "            comparison_values.append({'value': f1, \"type\": \"f1-score\", \"x\": removed_events_rel})\n",
    "            comparison_values.append({'value': prec, \"type\": \"precision\", \"x\": removed_events_rel})\n",
    "    # Add empty values to add space before IMf baseline values\n",
    "    # We add a \"//\" patch later, to indicate the axis break\n",
    "    values.append({'value': 0.0, \"type\": \"fitness\", \"x\": \"\"})\n",
    "    values.append({'value': 0.0, \"type\": \"f1-score\", \"x\": \"\"})\n",
    "    values.append({'value': 0.0, \"type\": \"precision\", \"x\": \"\"})\n",
    "    comparison_values.append({'value': 0.0, \"type\": \"fitness\", \"x\": \"\"})\n",
    "    comparison_values.append({'value': 0.0, \"type\": \"f1-score\", \"x\": \"\"})\n",
    "    comparison_values.append({'value': 0.0, \"type\": \"precision\", \"x\": \"\"})\n",
    "    # Add IMf as baseline comparison\n",
    "    imf_pickle_path = f\"/home/aarkue/doc/sciebo/alpha-revisit/final_figures/{log_to_IMf_res[log_name]}\"\n",
    "    with open(imf_pickle_path,'rb') as f:\n",
    "        p = pickle.load(f)\n",
    "        imf_data = p[\"IMf 0.2\"]\n",
    "        imf_fit = imf_data['fitness (alignment) %']/100\n",
    "        imf_prec = imf_data['precision (alignment) %']/100\n",
    "        imf_f1 = 2*(imf_prec * imf_fit)/(imf_prec + imf_fit)\n",
    "        print(f\"IMf 0.2 F1-Score: {round(imf_f1,4)}\")\n",
    "        values.append({'value': imf_fit, \"type\": \"fitness\", \"x\": \"IMf 0.2\"})\n",
    "        values.append({'value': imf_f1, \"type\": \"f1-score\", \"x\": \"IMf 0.2\"})\n",
    "        values.append({'value': imf_prec, \"type\": \"precision\", \"x\": \"IMf 0.2\"}) \n",
    "        comparison_values.append({'value': 0.0, \"type\": \"fitness\", \"x\": \"IMf 0.2\"})\n",
    "        comparison_values.append({'value': 0.0, \"type\": \"f1-score\", \"x\": \"IMf 0.2\"})\n",
    "        comparison_values.append({'value': 0.0, \"type\": \"precision\", \"x\": \"IMf 0.2\"})\n",
    "\n",
    "    df = pd.DataFrame(values, columns=[\"value\",\"type\",\"x\"])\n",
    "    fig,ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "    bar = sns.barplot(df,y=\"value\",x=\"x\", hue=\"type\",ax=ax, palette=MAIN_PALETTE, linestyle = \"-\", linewidth = 1, edgecolor = \"black\")\n",
    "\n",
    "    if compare_to is not None:\n",
    "        df_comparison = pd.DataFrame(comparison_values, columns=[\"value\",\"type\",\"x\"])\n",
    "        bar_comparison = sns.barplot(df_comparison,y=\"value\",x=\"x\", hue=\"type\",ax=ax, palette=ALT_PALETTE, alpha=0.4, fill=True)\n",
    "        bar_comparison = sns.barplot(df_comparison,y=\"value\",x=\"x\", hue=\"type\",ax=ax, alpha=1.0, linestyle = \":\", linewidth = 1, edgecolor = \"black\", fill=False) \n",
    "        plt.suptitle(\"Greedy and Brute-Force Activity Removal\",fontsize=24, y=1.025)\n",
    "\n",
    "    ax.set_xlabel(\"Number of Removed Activities // IMf 0.2 for comparison\", fontsize=14)\n",
    "    ax.set_ylabel(\"Fitness/Precision/F1-Score\")\n",
    "    ax.set_yticks([v/10 for v in range(0,11,1)])\n",
    "    ax.set_title(fig_title,fontsize=16, y=1.015)\n",
    "    t = plt.text(len(removing_disconnected_acts_data), -0.000, r'//', fontsize=18, zorder=100, horizontalalignment='center', verticalalignment='center',color=\"black\", backgroundcolor=\"white\")\n",
    "    # Build legend handles\n",
    "    # Starting with main elements\n",
    "    legend_handles = []\n",
    "    legend_handles += [matplotlib.patches.Patch(fill=False,linewidth=0,label=\"\")]\n",
    "    legend_handles += [matplotlib.patches.Patch(fill=False,linewidth=0,label=\"Greedy Mode\")]\n",
    "    legend_handles += [matplotlib.patches.Patch(color=MAIN_PALETTE[i],label=MEASURES[i]) for i in range(3)]\n",
    "    # + Comparison legend handles\n",
    "    if compare_to is not None:\n",
    "        legend_handles += [matplotlib.patches.Patch(fill=False,linewidth=0,label=\"\")]\n",
    "        legend_handles += [matplotlib.patches.Patch(fill=False,linewidth=0,label=\"Brute-Force Mode\\n  (Best F1-Score)\")]\n",
    "        legend_handles += [matplotlib.patches.Patch(facecolor=[c+\"35\" for c in ALT_PALETTE][i],linestyle=\":\", edgecolor = \"black\", linewidth=1,label=[m + \" (brute-force)\" for m in MEASURES][i]) for i in range(3)]\n",
    "    # Make + move legend\n",
    "    legend = plt.legend(handles=legend_handles,loc=1, title=\"Scores and Approaches\",title_fontsize=16)\n",
    "    sns.move_legend(ax,loc=(1.01, 0.33 ))\n",
    "    # Add explanation text\n",
    "    plt.text(min(0.4,0.1 * len(removing_disconnected_acts_data)/4),-0.25,\"\"\"Greedy mode: Remove disconnected labeled transitions in the reverse order of their log frequency, one by one.\n",
    "Brute-force mode: Consider all possible subsets of disconnected labeled transitions.\n",
    "For brute-force mode, the activity subset (of set size) which removal leads to the best F1-Score is considered.\"\"\", fontsize=12)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "# Helper to quickly generate comparison figures for all available results (dumped as json)\n",
    "for path in glob.glob(f\"{FIGURE_PATH}/removed-acts-results-*-brute-force.json\"):\n",
    "    # Retrieve log and algo name from filename\n",
    "    path_prefix = path[0:-len(\"-brute-force.json\")]\n",
    "    name: str = path_prefix.split(\"/\")[-1][len(\"removed-acts-results-\"):]\n",
    "    log_name = name[0:name.index(\"α\")-1]\n",
    "    algo_name = name[name.index(\"α\"):]\n",
    "    print(name)\n",
    "    brute_force_path = path\n",
    "    greedy_path = path_prefix + \"-greedy.json\"\n",
    "    with open(greedy_path, \"rt\") as f:\n",
    "        greedy_data: list[RemovingActsResult] = json.load(f)\n",
    "    with open(brute_force_path, \"rt\") as f:\n",
    "        brute_force_data: list[RemovingActsResult] = json.load(f)\n",
    "\n",
    "    best_brute_force = [\n",
    "        sorted([x for x in brute_force_data if len(x['removedActs']) == i],\n",
    "               key=lambda e: 2*(e['precision'] * e['fitness'])/(e['precision'] + e['fitness']), reverse=True)\n",
    "        [0] for i in range(len(greedy_data))]\n",
    "\n",
    "    fig_title = f\"Log: {log_name}, Algorithm: {algo_name}\"\n",
    "    fig: plt.Figure = plot_greedy_act_removal(\n",
    "        fig_title, greedy_data, best_brute_force)\n",
    "    if SAVE_FIGURES:\n",
    "        fig.savefig(FIGURE_PATH + name + \".svg\", bbox_inches='tight')\n",
    "    fig.show()\n",
    "\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
